{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.978 10.978  0.978  7.978  3.978  1.978  4.978  1.978  0.978 -0.022\n",
      "  8.978 10.978] 203.97799999999927\n",
      "[ 2.956 10.956  0.956  7.956  3.956  1.956  4.956  1.956  0.956 -0.044\n",
      "  8.956 10.956] 203.95599999999854\n",
      "[ 2.934 10.934  0.934  7.934  3.934  1.934  4.934  1.934  0.934 -0.066\n",
      "  8.934 10.934] 203.9339999999978\n",
      "[ 2.912 10.912  0.912  7.912  3.912  1.912  4.912  1.912  0.912 -0.088\n",
      "  8.912 10.912] 203.91199999999708\n",
      "[ 2.89 10.89  0.89  7.89  3.89  1.89  4.89  1.89  0.89 -0.11  8.89 10.89] 203.88999999999635\n",
      "[ 2.868 10.868  0.868  7.868  3.868  1.868  4.868  1.868  0.868 -0.132\n",
      "  8.868 10.868] 203.86799999999562\n",
      "[ 2.846 10.846  0.846  7.846  3.846  1.846  4.846  1.846  0.846 -0.154\n",
      "  8.846 10.846] 203.8459999999949\n",
      "[ 2.824 10.824  0.824  7.824  3.824  1.824  4.824  1.824  0.824 -0.176\n",
      "  8.824 10.824] 203.82399999999416\n",
      "[ 2.802 10.802  0.802  7.802  3.802  1.802  4.802  1.802  0.802 -0.198\n",
      "  8.802 10.802] 203.80199999999343\n",
      "[ 2.78 10.78  0.78  7.78  3.78  1.78  4.78  1.78  0.78 -0.22  8.78 10.78] 203.7799999999927\n",
      "[ 2.758 10.758  0.758  7.758  3.758  1.758  4.758  1.758  0.758 -0.242\n",
      "  8.758 10.758] 203.75799999999197\n",
      "[ 2.736 10.736  0.736  7.736  3.736  1.736  4.736  1.736  0.736 -0.264\n",
      "  8.736 10.736] 203.73599999999124\n",
      "[ 2.714 10.714  0.714  7.714  3.714  1.714  4.714  1.714  0.714 -0.286\n",
      "  8.714 10.714] 203.7139999999905\n",
      "[ 2.692 10.692  0.692  7.692  3.692  1.692  4.692  1.692  0.692 -0.308\n",
      "  8.692 10.692] 203.69199999998978\n",
      "[ 2.67 10.67  0.67  7.67  3.67  1.67  4.67  1.67  0.67 -0.33  8.67 10.67] 203.66999999998905\n",
      "[ 2.648 10.648  0.648  7.648  3.648  1.648  4.648  1.648  0.648 -0.352\n",
      "  8.648 10.648] 203.64799999998831\n",
      "[ 2.626 10.626  0.626  7.626  3.626  1.626  4.626  1.626  0.626 -0.374\n",
      "  8.626 10.626] 203.62599999998758\n",
      "[ 2.604 10.604  0.604  7.604  3.604  1.604  4.604  1.604  0.604 -0.396\n",
      "  8.604 10.604] 203.60399999998685\n",
      "[ 2.582 10.582  0.582  7.582  3.582  1.582  4.582  1.582  0.582 -0.418\n",
      "  8.582 10.582] 203.58199999998612\n",
      "[ 2.56 10.56  0.56  7.56  3.56  1.56  4.56  1.56  0.56 -0.44  8.56 10.56] 203.5599999999854\n",
      "[ 2.538 10.538  0.538  7.538  3.538  1.538  4.538  1.538  0.538 -0.462\n",
      "  8.538 10.538] 203.53799999998466\n",
      "[ 2.516 10.516  0.516  7.516  3.516  1.516  4.516  1.516  0.516 -0.484\n",
      "  8.516 10.516] 203.51599999998393\n",
      "[ 2.494 10.494  0.494  7.494  3.494  1.494  4.494  1.494  0.494 -0.506\n",
      "  8.494 10.494] 203.4939999999832\n",
      "[ 2.472 10.472  0.472  7.472  3.472  1.472  4.472  1.472  0.472 -0.528\n",
      "  8.472 10.472] 203.47199999998247\n",
      "[ 2.45 10.45  0.45  7.45  3.45  1.45  4.45  1.45  0.45 -0.55  8.45 10.45] 203.44999999998174\n",
      "[ 2.428 10.428  0.428  7.428  3.428  1.428  4.428  1.428  0.428 -0.572\n",
      "  8.428 10.428] 203.427999999981\n",
      "[ 2.406 10.406  0.406  7.406  3.406  1.406  4.406  1.406  0.406 -0.594\n",
      "  8.406 10.406] 203.40599999998028\n",
      "[ 2.384 10.384  0.384  7.384  3.384  1.384  4.384  1.384  0.384 -0.616\n",
      "  8.384 10.384] 203.38399999997955\n",
      "[ 2.362 10.362  0.362  7.362  3.362  1.362  4.362  1.362  0.362 -0.638\n",
      "  8.362 10.362] 203.36199999997882\n",
      "[ 2.34 10.34  0.34  7.34  3.34  1.34  4.34  1.34  0.34 -0.66  8.34 10.34] 203.3399999999781\n",
      "[ 2.318 10.318  0.318  7.318  3.318  1.318  4.318  1.318  0.318 -0.682\n",
      "  8.318 10.318] 203.31799999997736\n",
      "[ 2.296 10.296  0.296  7.296  3.296  1.296  4.296  1.296  0.296 -0.704\n",
      "  8.296 10.296] 203.29599999997663\n",
      "[ 2.274 10.274  0.274  7.274  3.274  1.274  4.274  1.274  0.274 -0.726\n",
      "  8.274 10.274] 203.2739999999759\n",
      "[ 2.252 10.252  0.252  7.252  3.252  1.252  4.252  1.252  0.252 -0.748\n",
      "  8.252 10.252] 203.25199999997517\n",
      "[ 2.23 10.23  0.23  7.23  3.23  1.23  4.23  1.23  0.23 -0.77  8.23 10.23] 203.22999999997444\n",
      "[ 2.208 10.208  0.208  7.208  3.208  1.208  4.208  1.208  0.208 -0.792\n",
      "  8.208 10.208] 203.2079999999737\n",
      "[ 2.186 10.186  0.186  7.186  3.186  1.186  4.186  1.186  0.186 -0.814\n",
      "  8.186 10.186] 203.18599999997298\n",
      "[ 2.164 10.164  0.164  7.164  3.164  1.164  4.164  1.164  0.164 -0.836\n",
      "  8.164 10.164] 203.16399999997225\n",
      "[ 2.142 10.142  0.142  7.142  3.142  1.142  4.142  1.142  0.142 -0.858\n",
      "  8.142 10.142] 203.14199999997152\n",
      "[ 2.12 10.12  0.12  7.12  3.12  1.12  4.12  1.12  0.12 -0.88  8.12 10.12] 203.1199999999708\n",
      "[ 2.098 10.098  0.098  7.098  3.098  1.098  4.098  1.098  0.098 -0.902\n",
      "  8.098 10.098] 203.09799999997006\n",
      "[ 2.076 10.076  0.076  7.076  3.076  1.076  4.076  1.076  0.076 -0.924\n",
      "  8.076 10.076] 203.07599999996933\n",
      "[ 2.054 10.054  0.054  7.054  3.054  1.054  4.054  1.054  0.054 -0.946\n",
      "  8.054 10.054] 203.0539999999686\n",
      "[ 2.032 10.032  0.032  7.032  3.032  1.032  4.032  1.032  0.032 -0.968\n",
      "  8.032 10.032] 203.03199999996787\n",
      "[ 2.010e+00  1.001e+01  1.000e-02  7.010e+00  3.010e+00  1.010e+00\n",
      "  4.010e+00  1.010e+00  1.000e-02 -9.900e-01  8.010e+00  1.001e+01] 203.00999999996714\n",
      "[ 1.988  9.988 -0.012  6.988  2.988  0.988  3.988  0.988 -0.012 -1.012\n",
      "  7.988  9.988] 202.9879999999664\n",
      "[ 1.966  9.966 -0.034  6.966  2.966  0.966  3.966  0.966 -0.034 -1.034\n",
      "  7.966  9.966] 202.96599999996567\n",
      "[ 1.944  9.944 -0.056  6.944  2.944  0.944  3.944  0.944 -0.056 -1.056\n",
      "  7.944  9.944] 202.94399999996494\n",
      "[ 1.922  9.922 -0.078  6.922  2.922  0.922  3.922  0.922 -0.078 -1.078\n",
      "  7.922  9.922] 202.9219999999642\n",
      "[ 1.9  9.9 -0.1  6.9  2.9  0.9  3.9  0.9 -0.1 -1.1  7.9  9.9] 202.89999999996348\n",
      "[ 1.878  9.878 -0.122  6.878  2.878  0.878  3.878  0.878 -0.122 -1.122\n",
      "  7.878  9.878] 202.87799999996275\n",
      "[ 1.856  9.856 -0.144  6.856  2.856  0.856  3.856  0.856 -0.144 -1.144\n",
      "  7.856  9.856] 202.85599999996202\n",
      "[ 1.834  9.834 -0.166  6.834  2.834  0.834  3.834  0.834 -0.166 -1.166\n",
      "  7.834  9.834] 202.8339999999613\n",
      "[ 1.812  9.812 -0.188  6.812  2.812  0.812  3.812  0.812 -0.188 -1.188\n",
      "  7.812  9.812] 202.81199999996056\n",
      "[ 1.79  9.79 -0.21  6.79  2.79  0.79  3.79  0.79 -0.21 -1.21  7.79  9.79] 202.78999999995983\n",
      "[ 1.768  9.768 -0.232  6.768  2.768  0.768  3.768  0.768 -0.232 -1.232\n",
      "  7.768  9.768] 202.7679999999591\n",
      "[ 1.746  9.746 -0.254  6.746  2.746  0.746  3.746  0.746 -0.254 -1.254\n",
      "  7.746  9.746] 202.74599999995837\n",
      "[ 1.724  9.724 -0.276  6.724  2.724  0.724  3.724  0.724 -0.276 -1.276\n",
      "  7.724  9.724] 202.72399999995764\n",
      "[ 1.702  9.702 -0.298  6.702  2.702  0.702  3.702  0.702 -0.298 -1.298\n",
      "  7.702  9.702] 202.7019999999569\n",
      "[ 1.68  9.68 -0.32  6.68  2.68  0.68  3.68  0.68 -0.32 -1.32  7.68  9.68] 202.67999999995618\n",
      "[ 1.658  9.658 -0.342  6.658  2.658  0.658  3.658  0.658 -0.342 -1.342\n",
      "  7.658  9.658] 202.65799999995545\n",
      "[ 1.636  9.636 -0.364  6.636  2.636  0.636  3.636  0.636 -0.364 -1.364\n",
      "  7.636  9.636] 202.63599999995472\n",
      "[ 1.614  9.614 -0.386  6.614  2.614  0.614  3.614  0.614 -0.386 -1.386\n",
      "  7.614  9.614] 202.613999999954\n",
      "[ 1.592  9.592 -0.408  6.592  2.592  0.592  3.592  0.592 -0.408 -1.408\n",
      "  7.592  9.592] 202.59199999995326\n",
      "[ 1.57  9.57 -0.43  6.57  2.57  0.57  3.57  0.57 -0.43 -1.43  7.57  9.57] 202.56999999995253\n",
      "[ 1.548  9.548 -0.452  6.548  2.548  0.548  3.548  0.548 -0.452 -1.452\n",
      "  7.548  9.548] 202.5479999999518\n",
      "[ 1.526  9.526 -0.474  6.526  2.526  0.526  3.526  0.526 -0.474 -1.474\n",
      "  7.526  9.526] 202.52599999995107\n",
      "[ 1.504  9.504 -0.496  6.504  2.504  0.504  3.504  0.504 -0.496 -1.496\n",
      "  7.504  9.504] 202.50399999995034\n",
      "[ 1.482  9.482 -0.518  6.482  2.482  0.482  3.482  0.482 -0.518 -1.518\n",
      "  7.482  9.482] 202.4819999999496\n",
      "[ 1.46  9.46 -0.54  6.46  2.46  0.46  3.46  0.46 -0.54 -1.54  7.46  9.46] 202.45999999994888\n",
      "[ 1.438  9.438 -0.562  6.438  2.438  0.438  3.438  0.438 -0.562 -1.562\n",
      "  7.438  9.438] 202.43799999994815\n",
      "[ 1.416  9.416 -0.584  6.416  2.416  0.416  3.416  0.416 -0.584 -1.584\n",
      "  7.416  9.416] 202.41599999994742\n",
      "[ 1.394  9.394 -0.606  6.394  2.394  0.394  3.394  0.394 -0.606 -1.606\n",
      "  7.394  9.394] 202.3939999999467\n",
      "[ 1.372  9.372 -0.628  6.372  2.372  0.372  3.372  0.372 -0.628 -1.628\n",
      "  7.372  9.372] 202.37199999994596\n",
      "[ 1.35  9.35 -0.65  6.35  2.35  0.35  3.35  0.35 -0.65 -1.65  7.35  9.35] 202.34999999994523\n",
      "[ 1.328  9.328 -0.672  6.328  2.328  0.328  3.328  0.328 -0.672 -1.672\n",
      "  7.328  9.328] 202.3279999999445\n",
      "[ 1.306  9.306 -0.694  6.306  2.306  0.306  3.306  0.306 -0.694 -1.694\n",
      "  7.306  9.306] 202.30599999994377\n",
      "[ 1.284  9.284 -0.716  6.284  2.284  0.284  3.284  0.284 -0.716 -1.716\n",
      "  7.284  9.284] 202.28399999994303\n",
      "[ 1.262  9.262 -0.738  6.262  2.262  0.262  3.262  0.262 -0.738 -1.738\n",
      "  7.262  9.262] 202.2619999999423\n",
      "[ 1.24  9.24 -0.76  6.24  2.24  0.24  3.24  0.24 -0.76 -1.76  7.24  9.24] 202.23999999994157\n",
      "[ 1.218  9.218 -0.782  6.218  2.218  0.218  3.218  0.218 -0.782 -1.782\n",
      "  7.218  9.218] 202.21799999994084\n",
      "[ 1.196  9.196 -0.804  6.196  2.196  0.196  3.196  0.196 -0.804 -1.804\n",
      "  7.196  9.196] 202.1959999999401\n",
      "[ 1.174  9.174 -0.826  6.174  2.174  0.174  3.174  0.174 -0.826 -1.826\n",
      "  7.174  9.174] 202.17399999993938\n",
      "[ 1.152  9.152 -0.848  6.152  2.152  0.152  3.152  0.152 -0.848 -1.848\n",
      "  7.152  9.152] 202.15199999993865\n",
      "[ 1.13  9.13 -0.87  6.13  2.13  0.13  3.13  0.13 -0.87 -1.87  7.13  9.13] 202.12999999993792\n",
      "[ 1.108  9.108 -0.892  6.108  2.108  0.108  3.108  0.108 -0.892 -1.892\n",
      "  7.108  9.108] 202.1079999999372\n",
      "[ 1.086  9.086 -0.914  6.086  2.086  0.086  3.086  0.086 -0.914 -1.914\n",
      "  7.086  9.086] 202.08599999993646\n",
      "[ 1.064  9.064 -0.936  6.064  2.064  0.064  3.064  0.064 -0.936 -1.936\n",
      "  7.064  9.064] 202.06399999993573\n",
      "[ 1.042  9.042 -0.958  6.042  2.042  0.042  3.042  0.042 -0.958 -1.958\n",
      "  7.042  9.042] 202.041999999935\n",
      "[ 1.02  9.02 -0.98  6.02  2.02  0.02  3.02  0.02 -0.98 -1.98  7.02  9.02] 202.01999999993427\n",
      "[ 9.980e-01  8.998e+00 -1.002e+00  5.998e+00  1.998e+00 -2.000e-03\n",
      "  2.998e+00 -2.000e-03 -1.002e+00 -2.002e+00  6.998e+00  8.998e+00] 201.99799999993354\n",
      "[ 0.976  8.976 -1.024  5.976  1.976 -0.024  2.976 -0.024 -1.024 -2.024\n",
      "  6.976  8.976] 201.9759999999328\n",
      "[ 0.954  8.954 -1.046  5.954  1.954 -0.046  2.954 -0.046 -1.046 -2.046\n",
      "  6.954  8.954] 201.95399999993208\n",
      "[ 0.932  8.932 -1.068  5.932  1.932 -0.068  2.932 -0.068 -1.068 -2.068\n",
      "  6.932  8.932] 201.93199999993135\n",
      "[ 0.91  8.91 -1.09  5.91  1.91 -0.09  2.91 -0.09 -1.09 -2.09  6.91  8.91] 201.90999999993062\n",
      "[ 0.888  8.888 -1.112  5.888  1.888 -0.112  2.888 -0.112 -1.112 -2.112\n",
      "  6.888  8.888] 201.8879999999299\n",
      "[ 0.866  8.866 -1.134  5.866  1.866 -0.134  2.866 -0.134 -1.134 -2.134\n",
      "  6.866  8.866] 201.86599999992916\n",
      "[ 0.844  8.844 -1.156  5.844  1.844 -0.156  2.844 -0.156 -1.156 -2.156\n",
      "  6.844  8.844] 201.84399999992843\n",
      "[ 0.822  8.822 -1.178  5.822  1.822 -0.178  2.822 -0.178 -1.178 -2.178\n",
      "  6.822  8.822] 201.8219999999277\n",
      "[ 0.8  8.8 -1.2  5.8  1.8 -0.2  2.8 -0.2 -1.2 -2.2  6.8  8.8] 201.79999999992697\n",
      "The local minimum occurs at: Omega=[ 0.8  8.8 -1.2  5.8  1.8 -0.2  2.8 -0.2 -1.2 -2.2  6.8  8.8] Beta=201.79999999992697\n",
      "Reported accuracy:48.0%\n",
      "Run time is:  262.52962660000003\n",
      "(array([ 0.8,  8.8, -1.2,  5.8,  1.8, -0.2,  2.8, -0.2, -1.2, -2.2,  6.8,\n",
      "        8.8]), None, None, None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import timeit\n",
    "from scipy.special import expit\n",
    "\n",
    "data = np.loadtxt('heart.dat')\n",
    "\n",
    "for k in range(len(data)):\n",
    "    if data[k][13]==2:\n",
    "        data[k][13]=1 \n",
    "    else:\n",
    "        data[k][13]=-1\n",
    "        \n",
    "data_x = data[:220]\n",
    "data_t = data[220:]\n",
    "dtype_ = np.dtype(dtype=\"float64\")\n",
    "\n",
    "input_data=[]\n",
    "\n",
    "for r in range(len(data_x)):\n",
    "    input_data.append([data_x[r][:12],data_x[r][13:]])\n",
    "\n",
    "testing_data=[]\n",
    "\n",
    "for s in range(len(data_t)):\n",
    "    testing_data.append([data_t[s][:12],data_t[s][13:]])\n",
    "\n",
    "#Setting up the data type for numpy arrays\n",
    "dtype=\"float64\"\n",
    "dtype_ = np.dtype(dtype)\n",
    "x_i = list()\n",
    "y_i = list()\n",
    "for i, j in input_data:\n",
    "        x_i.append(i)\n",
    "        y_i.append(j)\n",
    "x_i, y_i = np.array(x_i, dtype=dtype_), np.array(y_i, dtype=dtype_)\n",
    "\n",
    "#Initializing the values of the Omega and Beta\n",
    "start_data_w= np.random.randint(np.size(x_i[0]), size=np.size(x_i[0]))\n",
    "start_data_b = np.random.randint(np.size(y_i))\n",
    "\n",
    "#Gradient function\n",
    "values = []\n",
    "def gradient(training,W,B):\n",
    "    L = 0.0001\n",
    "    m = np.size(W)\n",
    "    a = np.zeros(1)\n",
    "    s = np.zeros(m+1)\n",
    "    b = np.append(W,a)\n",
    "\n",
    "    G0 = 2*L*b\n",
    "    \n",
    "    for x,y in input_data:\n",
    "        c = np.array(np.sum(-y*x), dtype=dtype)\n",
    "        d = np.array([[-y]], dtype=dtype)\n",
    "        e = np.array([[c],d], dtype=dtype)\n",
    "\n",
    "        if np.add(1,np.sum(-y*np.add(np.sum(W*x),B)))<=0:\n",
    "            values.append([[np.add(G0,s)]])\n",
    "\n",
    "        elif np.add(1,np.sum(-y*np.add(np.sum(W*x),B)))>0:\n",
    "            values.append([[np.add(G0,e)]])\n",
    "\n",
    "def ssgm(gradient, training_d, start_w, start_b, stepsize, tolerance, n_iter, batch_size,\n",
    "        dtype=\"float64\", random_state=None):\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    #Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    #Setting up the data type for numpy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    #Converting x and y to numpy arrays\n",
    "    n_obs = x_i.shape[0]\n",
    "    if n_obs != y_i.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    x_iy_i = np.c_[x_i.reshape(n_obs, -1), y_i.reshape(n_obs, 1)]\n",
    "    \n",
    "    #Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    #Initializing the values of the variables\n",
    "    StartingV = [start_w, start_b]\n",
    "    vector = np.array(start_w, dtype=dtype_)\n",
    "    beta = np.array(start_b, dtype=dtype_)\n",
    "\n",
    "    #Setting up and checking the stepsize\n",
    "    stepsize = np.array(stepsize, dtype=dtype_)\n",
    "    \n",
    "    if np.any(stepsize <= 0):\n",
    "        raise ValueError(\"'stepsize' must be greater than zero\")\n",
    "\n",
    "    #Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "\n",
    "    #Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    #Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "    \n",
    "    #Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        #Shuffle x and y\n",
    "        rng.shuffle(x_iy_i)\n",
    "\n",
    "        #Performing minibatch moves\n",
    "        for StartingV in range(0, n_obs, batch_size):\n",
    "            stop_t = StartingV + batch_size\n",
    "            x_batch, y_batch = x_iy_i[StartingV:stop_t, :-1], x_iy_i[StartingV:stop_t, -1:]\n",
    "\n",
    "            #Recalculating the difference\n",
    "            diff = np.sum(-stepsize,gradient([x_batch, y_batch], vector, beta))\n",
    "\n",
    "            #Checking if the absolute difference is small enough\n",
    "            if (abs(diff.all()) > tolerance):\n",
    "                #Updating the values of the variables\n",
    "                vector+=diff\n",
    "                beta+=diff\n",
    "            else:\n",
    "                print(\"break\",start,stop)\n",
    "                break\n",
    "                \n",
    "        print(vector,beta)           \n",
    "    count_accuracy = 0\n",
    "    \n",
    "    for h,l in testing_data:\n",
    "        if np.add(np.sum(np.array(vector)*np.array(h)),np.array(beta))>=0 and np.array(l)==1:\n",
    "            count_accuracy += 1\n",
    "\n",
    "        elif np.add(np.sum(np.array(vector)*np.array(h)),np.array(beta))<0 and np.array(l)==-1:\n",
    "            count_accuracy += 1\n",
    "            \n",
    "        check_accuracy = np.divide(count_accuracy,len(testing_data))*100\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    return(vector if vector.shape else vector.item(), \n",
    "           print(\"The local minimum occurs at: Omega={}\".format(vector), \"Beta={}\".format(beta)),\n",
    "          print(\"Reported accuracy:{}%\".format(check_accuracy)), print('Run time is: ', stop-start))\n",
    "\n",
    "print(ssgm(gradient, input_data, start_data_w, start_data_b, 0.0001, 1e-04, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
