{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.9501 1.9501 1.9501 5.9501 5.9501 1.9501 3.9501] 42.95009999999834\n",
      "[4.9002 1.9002 1.9002 5.9002 5.9002 1.9002 3.9002] 42.90019999999669\n",
      "[4.8503 1.8503 1.8503 5.8503 5.8503 1.8503 3.8503] 42.85029999999503\n",
      "[4.8004 1.8004 1.8004 5.8004 5.8004 1.8004 3.8004] 42.800399999993374\n",
      "[4.7505 1.7505 1.7505 5.7505 5.7505 1.7505 3.7505] 42.75049999999172\n",
      "[4.7006 1.7006 1.7006 5.7006 5.7006 1.7006 3.7006] 42.70059999999006\n",
      "[4.6507 1.6507 1.6507 5.6507 5.6507 1.6507 3.6507] 42.650699999988404\n",
      "[4.6008 1.6008 1.6008 5.6008 5.6008 1.6008 3.6008] 42.60079999998675\n",
      "[4.5509 1.5509 1.5509 5.5509 5.5509 1.5509 3.5509] 42.55089999998509\n",
      "[4.501 1.501 1.501 5.501 5.501 1.501 3.501] 42.500999999983435\n",
      "[4.4511 1.4511 1.4511 5.4511 5.4511 1.4511 3.4511] 42.45109999998178\n",
      "[4.4012 1.4012 1.4012 5.4012 5.4012 1.4012 3.4012] 42.40119999998012\n",
      "[4.3513 1.3513 1.3513 5.3513 5.3513 1.3513 3.3513] 42.351299999978465\n",
      "[4.3014 1.3014 1.3014 5.3014 5.3014 1.3014 3.3014] 42.30139999997681\n",
      "[4.2515 1.2515 1.2515 5.2515 5.2515 1.2515 3.2515] 42.25149999997515\n",
      "[4.2016 1.2016 1.2016 5.2016 5.2016 1.2016 3.2016] 42.201599999973496\n",
      "[4.1517 1.1517 1.1517 5.1517 5.1517 1.1517 3.1517] 42.15169999997184\n",
      "[4.1018 1.1018 1.1018 5.1018 5.1018 1.1018 3.1018] 42.10179999997018\n",
      "[4.0519 1.0519 1.0519 5.0519 5.0519 1.0519 3.0519] 42.051899999968526\n",
      "[4.002 1.002 1.002 5.002 5.002 1.002 3.002] 42.00199999996687\n",
      "[3.9521 0.9521 0.9521 4.9521 4.9521 0.9521 2.9521] 41.95209999996521\n",
      "[3.9022 0.9022 0.9022 4.9022 4.9022 0.9022 2.9022] 41.90219999996356\n",
      "[3.8523 0.8523 0.8523 4.8523 4.8523 0.8523 2.8523] 41.8522999999619\n",
      "[3.8024 0.8024 0.8024 4.8024 4.8024 0.8024 2.8024] 41.802399999960244\n",
      "[3.7525 0.7525 0.7525 4.7525 4.7525 0.7525 2.7525] 41.75249999995859\n",
      "[3.7026 0.7026 0.7026 4.7026 4.7026 0.7026 2.7026] 41.70259999995693\n",
      "[3.6527 0.6527 0.6527 4.6527 4.6527 0.6527 2.6527] 41.652699999955274\n",
      "[3.6028 0.6028 0.6028 4.6028 4.6028 0.6028 2.6028] 41.60279999995362\n",
      "[3.5529 0.5529 0.5529 4.5529 4.5529 0.5529 2.5529] 41.55289999995196\n",
      "[3.503 0.503 0.503 4.503 4.503 0.503 2.503] 41.502999999950305\n",
      "[3.4531 0.4531 0.4531 4.4531 4.4531 0.4531 2.4531] 41.45309999994865\n",
      "[3.4032 0.4032 0.4032 4.4032 4.4032 0.4032 2.4032] 41.40319999994699\n",
      "[3.3533 0.3533 0.3533 4.3533 4.3533 0.3533 2.3533] 41.353299999945335\n",
      "[3.3034 0.3034 0.3034 4.3034 4.3034 0.3034 2.3034] 41.30339999994368\n",
      "[3.2535 0.2535 0.2535 4.2535 4.2535 0.2535 2.2535] 41.25349999994202\n",
      "[3.2036 0.2036 0.2036 4.2036 4.2036 0.2036 2.2036] 41.203599999940366\n",
      "[3.1537 0.1537 0.1537 4.1537 4.1537 0.1537 2.1537] 41.15369999993871\n",
      "[3.1038 0.1038 0.1038 4.1038 4.1038 0.1038 2.1038] 41.10379999993705\n",
      "[3.0539 0.0539 0.0539 4.0539 4.0539 0.0539 2.0539] 41.053899999935396\n",
      "[3.004e+00 4.000e-03 4.000e-03 4.004e+00 4.004e+00 4.000e-03 2.004e+00] 41.00399999993374\n",
      "[ 2.9541 -0.0459 -0.0459  3.9541  3.9541 -0.0459  1.9541] 40.95409999993208\n",
      "[ 2.9042 -0.0958 -0.0958  3.9042  3.9042 -0.0958  1.9042] 40.90419999993043\n",
      "[ 2.8543 -0.1457 -0.1457  3.8543  3.8543 -0.1457  1.8543] 40.85429999992877\n",
      "[ 2.8044 -0.1956 -0.1956  3.8044  3.8044 -0.1956  1.8044] 40.804399999927114\n",
      "[ 2.7545 -0.2455 -0.2455  3.7545  3.7545 -0.2455  1.7545] 40.75449999992546\n",
      "[ 2.7046 -0.2954 -0.2954  3.7046  3.7046 -0.2954  1.7046] 40.7045999999238\n",
      "[ 2.6547 -0.3453 -0.3453  3.6547  3.6547 -0.3453  1.6547] 40.654699999922144\n",
      "[ 2.6048 -0.3952 -0.3952  3.6048  3.6048 -0.3952  1.6048] 40.60479999992049\n",
      "[ 2.5549 -0.4451 -0.4451  3.5549  3.5549 -0.4451  1.5549] 40.55489999991883\n",
      "[ 2.505 -0.495 -0.495  3.505  3.505 -0.495  1.505] 40.504999999917175\n",
      "[ 2.4551 -0.5449 -0.5449  3.4551  3.4551 -0.5449  1.4551] 40.45509999991552\n",
      "[ 2.4052 -0.5948 -0.5948  3.4052  3.4052 -0.5948  1.4052] 40.40519999991386\n",
      "[ 2.3553 -0.6447 -0.6447  3.3553  3.3553 -0.6447  1.3553] 40.355299999912205\n",
      "[ 2.3054 -0.6946 -0.6946  3.3054  3.3054 -0.6946  1.3054] 40.30539999991055\n",
      "[ 2.2555 -0.7445 -0.7445  3.2555  3.2555 -0.7445  1.2555] 40.25549999990889\n",
      "[ 2.2056 -0.7944 -0.7944  3.2056  3.2056 -0.7944  1.2056] 40.205599999907236\n",
      "[ 2.1557 -0.8443 -0.8443  3.1557  3.1557 -0.8443  1.1557] 40.15569999990558\n",
      "[ 2.1058 -0.8942 -0.8942  3.1058  3.1058 -0.8942  1.1058] 40.10579999990392\n",
      "[ 2.0559 -0.9441 -0.9441  3.0559  3.0559 -0.9441  1.0559] 40.055899999902266\n",
      "[ 2.006 -0.994 -0.994  3.006  3.006 -0.994  1.006] 40.00599999990061\n",
      "[ 1.9561 -1.0439 -1.0439  2.9561  2.9561 -1.0439  0.9561] 39.95609999989895\n",
      "[ 1.9062 -1.0938 -1.0938  2.9062  2.9062 -1.0938  0.9062] 39.9061999998973\n",
      "[ 1.8563 -1.1437 -1.1437  2.8563  2.8563 -1.1437  0.8563] 39.85629999989564\n",
      "[ 1.8064 -1.1936 -1.1936  2.8064  2.8064 -1.1936  0.8064] 39.80639999989398\n",
      "[ 1.7565 -1.2435 -1.2435  2.7565  2.7565 -1.2435  0.7565] 39.75649999989233\n",
      "[ 1.7066 -1.2934 -1.2934  2.7066  2.7066 -1.2934  0.7066] 39.70659999989067\n",
      "[ 1.6567 -1.3433 -1.3433  2.6567  2.6567 -1.3433  0.6567] 39.656699999889014\n",
      "[ 1.6068 -1.3932 -1.3932  2.6068  2.6068 -1.3932  0.6068] 39.60679999988736\n",
      "[ 1.5569 -1.4431 -1.4431  2.5569  2.5569 -1.4431  0.5569] 39.5568999998857\n",
      "[ 1.507 -1.493 -1.493  2.507  2.507 -1.493  0.507] 39.506999999884044\n",
      "[ 1.4571 -1.5429 -1.5429  2.4571  2.4571 -1.5429  0.4571] 39.45709999988239\n",
      "[ 1.4072 -1.5928 -1.5928  2.4072  2.4072 -1.5928  0.4072] 39.40719999988073\n",
      "[ 1.3573 -1.6427 -1.6427  2.3573  2.3573 -1.6427  0.3573] 39.357299999879075\n",
      "[ 1.3074 -1.6926 -1.6926  2.3074  2.3074 -1.6926  0.3074] 39.30739999987742\n",
      "[ 1.2575 -1.7425 -1.7425  2.2575  2.2575 -1.7425  0.2575] 39.25749999987576\n",
      "[ 1.2076 -1.7924 -1.7924  2.2076  2.2076 -1.7924  0.2076] 39.207599999874105\n",
      "[ 1.1577 -1.8423 -1.8423  2.1577  2.1577 -1.8423  0.1577] 39.15769999987245\n",
      "[ 1.1078 -1.8922 -1.8922  2.1078  2.1078 -1.8922  0.1078] 39.10779999987079\n",
      "[ 1.0579 -1.9421 -1.9421  2.0579  2.0579 -1.9421  0.0579] 39.057899999869136\n",
      "[ 1.008 -1.992 -1.992  2.008  2.008 -1.992  0.008] 39.00799999986748\n",
      "[ 0.9581 -2.0419 -2.0419  1.9581  1.9581 -2.0419 -0.0419] 38.95809999986582\n",
      "[ 0.9082 -2.0918 -2.0918  1.9082  1.9082 -2.0918 -0.0918] 38.908199999864166\n",
      "[ 0.8583 -2.1417 -2.1417  1.8583  1.8583 -2.1417 -0.1417] 38.85829999986251\n",
      "[ 0.8084 -2.1916 -2.1916  1.8084  1.8084 -2.1916 -0.1916] 38.80839999986085\n",
      "[ 0.7585 -2.2415 -2.2415  1.7585  1.7585 -2.2415 -0.2415] 38.7584999998592\n",
      "[ 0.7086 -2.2914 -2.2914  1.7086  1.7086 -2.2914 -0.2914] 38.70859999985754\n",
      "[ 0.6587 -2.3413 -2.3413  1.6587  1.6587 -2.3413 -0.3413] 38.658699999855884\n",
      "[ 0.6088 -2.3912 -2.3912  1.6088  1.6088 -2.3912 -0.3912] 38.60879999985423\n",
      "[ 0.5589 -2.4411 -2.4411  1.5589  1.5589 -2.4411 -0.4411] 38.55889999985257\n",
      "[ 0.509 -2.491 -2.491  1.509  1.509 -2.491 -0.491] 38.508999999850914\n",
      "[ 0.4591 -2.5409 -2.5409  1.4591  1.4591 -2.5409 -0.5409] 38.45909999984926\n",
      "[ 0.4092 -2.5908 -2.5908  1.4092  1.4092 -2.5908 -0.5908] 38.4091999998476\n",
      "[ 0.3593 -2.6407 -2.6407  1.3593  1.3593 -2.6407 -0.6407] 38.359299999845945\n",
      "[ 0.3094 -2.6906 -2.6906  1.3094  1.3094 -2.6906 -0.6906] 38.30939999984429\n",
      "[ 0.2595 -2.7405 -2.7405  1.2595  1.2595 -2.7405 -0.7405] 38.25949999984263\n",
      "[ 0.2096 -2.7904 -2.7904  1.2096  1.2096 -2.7904 -0.7904] 38.209599999840975\n",
      "[ 0.1597 -2.8403 -2.8403  1.1597  1.1597 -2.8403 -0.8403] 38.15969999983932\n",
      "[ 0.1098 -2.8902 -2.8902  1.1098  1.1098 -2.8902 -0.8902] 38.10979999983766\n",
      "[ 0.0599 -2.9401 -2.9401  1.0599  1.0599 -2.9401 -0.9401] 38.059899999836006\n",
      "[ 0.01 -2.99 -2.99  1.01  1.01 -2.99 -0.99] 38.00999999983435\n",
      "The local minimum occurs at: Omega=[ 0.01 -2.99 -2.99  1.01  1.01 -2.99 -0.99] Beta=38.00999999983435\n",
      "Reported accuracy:68.02973977695167%\n",
      "Run time is:  1382.0009807\n",
      "(array([ 0.01, -2.99, -2.99,  1.01,  1.01, -2.99, -0.99]), None, None, None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import timeit\n",
    "from scipy.special import expit\n",
    "\n",
    "data = np.genfromtxt('diabetes.csv',delimiter=',')\n",
    "for k in range(len(data)):\n",
    "    if data[k][8]==0:\n",
    "        data[k][8]=-1\n",
    "        \n",
    "data_x = data[1:500]\n",
    "data_t = data[500:]\n",
    "\n",
    "dtype_ = np.dtype(dtype=\"float64\")\n",
    "\n",
    "input_data=[]\n",
    "\n",
    "for r in range(len(data_x)):\n",
    "    input_data.append([data_x[r][:7],data_x[r][8:]])\n",
    "\n",
    "testing_data=[]\n",
    "\n",
    "for s in range(len(data_t)):\n",
    "    testing_data.append([data_t[s][:7],data_t[s][8:]])\n",
    "\n",
    "#Setting up the data type for numpy arrays\n",
    "dtype=\"float64\"\n",
    "dtype_ = np.dtype(dtype)\n",
    "x_i = list()\n",
    "y_i = list()\n",
    "for i, j in input_data:\n",
    "        x_i.append(i)\n",
    "        y_i.append(j)\n",
    "x_i, y_i = np.array(x_i, dtype=dtype_), np.array(y_i, dtype=dtype_)\n",
    "\n",
    "#Initializing the values of the Omega and Beta\n",
    "start_data_w= np.random.randint(np.size(x_i[0]), size=np.size(x_i[0]))\n",
    "start_data_b = np.random.randint(np.size(y_i))\n",
    "\n",
    "#Gradient function\n",
    "values = []\n",
    "def gradient(training,W,B):\n",
    "    L = 0.0001\n",
    "    m = np.size(W)\n",
    "    a = np.zeros(1)\n",
    "    s = np.zeros(m+1)\n",
    "    b = np.append(W,a)\n",
    "\n",
    "    G0 = 2*L*b\n",
    "    \n",
    "    for x,y in input_data:\n",
    "        c = np.array(np.sum(-y*x), dtype=dtype)\n",
    "        d = np.array([[-y]], dtype=dtype)\n",
    "        e = np.array([[c],d], dtype=dtype)\n",
    "\n",
    "        if np.add(1,np.sum(-y*np.add(np.sum(W*x),B)))<=0:\n",
    "            values.append([[np.add(G0,s)]])\n",
    "\n",
    "        elif np.add(1,np.sum(-y*np.add(np.sum(W*x),B)))>0:\n",
    "            values.append([[np.add(G0,e)]])\n",
    "\n",
    "def ssgm(gradient, training_d, start_w, start_b, stepsize, tolerance, n_iter, batch_size,\n",
    "        dtype=\"float64\", random_state=None):\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    #Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    #Setting up the data type for numpy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    #Converting x and y to numpy arrays\n",
    "    n_obs = x_i.shape[0]\n",
    "    if n_obs != y_i.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "    x_iy_i = np.c_[x_i.reshape(n_obs, -1), y_i.reshape(n_obs, 1)]\n",
    "    \n",
    "    #Initializing the random number generator\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    #Initializing the values of the variables\n",
    "    StartingV = [start_w, start_b]\n",
    "    vector = np.array(start_w, dtype=dtype_)\n",
    "    beta = np.array(start_b, dtype=dtype_)\n",
    "\n",
    "    #Setting up and checking the stepsize\n",
    "    stepsize = np.array(stepsize, dtype=dtype_)\n",
    "    \n",
    "    if np.any(stepsize <= 0):\n",
    "        raise ValueError(\"'stepsize' must be greater than zero\")\n",
    "\n",
    "    #Setting up and checking the size of minibatches\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\n",
    "            \"'batch_size' must be greater than zero and less than \"\n",
    "            \"or equal to the number of observations\"\n",
    "        )\n",
    "\n",
    "    #Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    #Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "    \n",
    "    #Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        #Shuffle x and y\n",
    "        rng.shuffle(x_iy_i)\n",
    "\n",
    "        #Performing minibatch moves\n",
    "        for StartingV in range(0, n_obs, batch_size):\n",
    "            stop_t = StartingV + batch_size\n",
    "            x_batch, y_batch = x_iy_i[StartingV:stop_t, :-1], x_iy_i[StartingV:stop_t, -1:]\n",
    "\n",
    "            #Recalculating the difference\n",
    "            diff = np.sum(-stepsize,gradient([x_batch, y_batch], vector, beta))\n",
    "\n",
    "            #Checking if the absolute difference is small enough\n",
    "            if (abs(diff.all()) > tolerance):\n",
    "                #Updating the values of the variables\n",
    "                vector+=diff\n",
    "                beta+=diff\n",
    "            else:\n",
    "                print(\"break\",start,stop)\n",
    "                break\n",
    "                \n",
    "        print(vector,beta)           \n",
    "    count_accuracy = 0\n",
    "    \n",
    "    for h,l in testing_data:\n",
    "        if np.add(np.sum(np.array(vector)*np.array(h)),np.array(beta))>=0 and np.array(l)==1:\n",
    "            count_accuracy += 1\n",
    "\n",
    "        elif np.add(np.sum(np.array(vector)*np.array(h)),np.array(beta))<0 and np.array(l)==-1:\n",
    "            count_accuracy += 1\n",
    "            \n",
    "        check_accuracy = np.divide(count_accuracy,len(testing_data))*100\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    return(vector if vector.shape else vector.item(), \n",
    "           print(\"The local minimum occurs at: Omega={}\".format(vector), \"Beta={}\".format(beta)),\n",
    "          print(\"Reported accuracy:{}%\".format(check_accuracy)), print('Run time is: ', stop-start))\n",
    "\n",
    "print(ssgm(gradient, input_data, start_data_w, start_data_b, 0.0001, 1e-04, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
