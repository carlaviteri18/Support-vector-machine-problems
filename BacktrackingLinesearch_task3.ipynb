{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import timeit\n",
    "from scipy.special import expit\n",
    "\n",
    "#We will update the code depending on the data file in question\n",
    "#Using the data file in question\n",
    "\n",
    "#Line to import a .dat file\n",
    "data = np.loadtxt('.dat file')\n",
    "\n",
    "#Line to import a .csv file\n",
    "data = np.genfromtxt('.csv file',delimiter=',')\n",
    "\n",
    "for k in range(len(data)):\n",
    "    #Where x represents the last column of the array\n",
    "    if data[k][x]==2:\n",
    "        data[k][x]=1 \n",
    "    else:\n",
    "        data[k][x]=-1\n",
    "\n",
    "#Where x represents the first m rows, followed by the remaining rows in the data set, respectively\n",
    "data_x = data[:x]\n",
    "data_t = data[x:]\n",
    "dtype_ = np.dtype(dtype=\"float64\")\n",
    "\n",
    "input_data=[]\n",
    "\n",
    "#Where i and j represent the first n-1 columns, followed by the last column of the array, respectively\n",
    "for r in range(len(data_x)):\n",
    "    input_data.append([data_x[r][:i],data_x[r][j:]])\n",
    "\n",
    "testing_data=[]\n",
    "\n",
    "for s in range(len(data_t)):\n",
    "    testing_data.append([data_t[s][:i],data_t[s][j:]])\n",
    "\n",
    "#Setting up the data type for numpy arrays\n",
    "dtype=\"float64\"\n",
    "dtype_ = np.dtype(dtype)\n",
    "x_i = list()\n",
    "y_i = list()\n",
    "for i, j in input_data:\n",
    "        x_i.append(i)\n",
    "        y_i.append(j)\n",
    "x_i, y_i = np.array(x_i, dtype=dtype_), np.array(y_i, dtype=dtype_)\n",
    "\n",
    "#Initializing the values of the Omega and Beta\n",
    "start_data_w= np.random.randint(np.size(x_i[0]), size=np.size(x_i[0]))\n",
    "start_data_b = np.random.randint(np.size(y_i))\n",
    "Alpha = 0.5\n",
    "Beta = 0.5\n",
    "\n",
    "#Gradient function\n",
    "def gradient(training,W,B):\n",
    "    L = 0.0001\n",
    "    m = np.size(W)\n",
    "    G0 = 2*L*W\n",
    "    \n",
    "    for x,y in input_data:\n",
    "        #New array with the specified precision\n",
    "        z = np.sum(-y*(np.sum(W*x[:, None]) + B))\n",
    "        z = expit(z)\n",
    "        sumgrad = np.subtract(np.sum(np.sum(-y*x)*np.exp(z)),np.sum(np.sum(y*np.exp(z))))/np.add(1,np.exp(np.sum(-y*np.exp(z))))\n",
    "    return(np.add(G0,(1/m)*np.sum(sumgrad)))\n",
    "\n",
    "def function_x(training_f, W, B):\n",
    "    L = 0.0001\n",
    "    m = np.size(W)\n",
    "    \n",
    "    for x,y in input_data:\n",
    "        #New array with the specified precision\n",
    "        z = np.sum(-y*(np.sum(W*x[:, None]) + B))\n",
    "        z = expit(z)\n",
    "        sumfun = np.log(1+np.exp(z))\n",
    "    return(L*(np.linalg.norm(W))**2+(1/m)*np.sum(sumfun))            \n",
    "\n",
    "def gradient_descent(\n",
    "    gradient, training_d, start_w, start_b, stepsize, n_iter, tolerance,\n",
    "    dtype=\"float64\"\n",
    "):\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    #Checking if the gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable\")\n",
    "\n",
    "    #Setting up the data type for numpy arrays\n",
    "    dtype_ = np.dtype(dtype)\n",
    "\n",
    "    #Converting x and y to numpy arrays\n",
    "    x = list()\n",
    "    y = list()\n",
    "    for i, j in input_data:\n",
    "        x.append(i)\n",
    "        y.append(j)\n",
    "        \n",
    "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "\n",
    "    #Initializing the values of the variables\n",
    "    vector = np.array(start_w, dtype=dtype_)\n",
    "    beta = np.array(start_b, dtype=dtype_)\n",
    "\n",
    "    #Setting up and checking the stepsize\n",
    "    stepsize = np.array(stepsize, dtype=dtype_)\n",
    "    if np.any(stepsize <= 0):\n",
    "        raise ValueError(\"'stepsize' must be greater than zero\")\n",
    "\n",
    "    #Setting up and checking the maximal number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
    "\n",
    "    #Setting up and checking the tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
    "    \n",
    "    #Performing the gradient descent loop\n",
    "    for _ in range(n_iter):\n",
    "        #Recalculating the difference\n",
    "        diff = -stepsize* np.array(gradient(training_d, vector, beta), dtype_)\n",
    "\n",
    "        #Checking if the absolute difference is small enough and recalculating the step size\n",
    "        if np.all(np.subtract(np.array(function_x(training_d, start_w, start_b), dtype_),np.array(function_x(diff, vector, beta), dtype_)))<Alpha*stepsize*np.linalg.norm(np.array(gradient(diff, vector, beta), dtype_))**2:\n",
    "            #Updating the values of the variables\n",
    "            stepsize = Beta*stepsize\n",
    "        else:\n",
    "            print(\"break\",start,stop)\n",
    "            break\n",
    "            \n",
    "        if np.all(np.abs(diff) > tolerance):\n",
    "            vector, beta = (val + delta for val, delta in zip((vector, beta), diff))\n",
    "        else:\n",
    "            print(\"break\",start,stop)\n",
    "            break\n",
    "            \n",
    "        print(vector,beta)    \n",
    "    count_accuracy = 0\n",
    "    \n",
    "    for h,l in testing_data:\n",
    "        if np.add(np.sum(np.array(vector)*np.array(h)),np.array(beta))>=0 and np.array(l)==1:\n",
    "            count_accuracy += 1\n",
    "\n",
    "        elif np.add(np.sum(np.array(vector)*np.array(h)),np.array(beta))<0 and np.array(l)==-1:\n",
    "            count_accuracy += 1\n",
    "            \n",
    "        check_accuracy = np.divide(count_accuracy,len(testing_data))*100\n",
    "        \n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    return(vector if vector.shape else vector.item(), \n",
    "           print(\"The local minimum occurs at: Omega={}\".format(vector), \"Beta={}\".format(beta)),\n",
    "          print(\"Reported accuracy:{}%\".format(check_accuracy)), print('Run time is: ', stop-start))\n",
    "\n",
    "print(gradient_descent(gradient, input_data, start_data_w, start_data_b, 1, 500, 1e-04))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
